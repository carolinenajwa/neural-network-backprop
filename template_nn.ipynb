{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp/ejO3WsrLbNC8dW6z+Rc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Implementation (Inspired from):**\n",
        "- \"Neural Network from scratch in Python\" by Omar Aflak: [Link](https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65)\n",
        "\n",
        "\n",
        "\n",
        "This Python program implements a neural network with adjustments inspired by the code\n",
        "from the above-mentioned source. The original code served as a foundational reference,\n",
        "and modifications have been made to suit specific requirements and preferences."
      ],
      "metadata": {
        "id": "pk7hwbs_E4zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "IUgysjY_6vmN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "98kL0m3JcbHu"
      },
      "outputs": [],
      "source": [
        "# Abstract base class to handle input, output, forward prop., backward prop.\n",
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "    def forward_propagation(self, input):\n",
        "      raise NotImplementedError\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "      raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Fully Connected (FC) - inherits from base class Layer\n",
        "class FCLayer(Layer):\n",
        "  # input_size = number of input neurons\n",
        "  # output_size = number of output nerons\n",
        "\n",
        "  def __init__(self, input_size, output_size):\n",
        "    # Initialize weights and biases\n",
        "    # Weights are initialized randomly to break symmetry\n",
        "    # create weight vector\n",
        "    self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "     # create bias vector\n",
        "    self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    # Forward Propagation returns output for a given input\n",
        "  def forward_propagation(self, input_data):\n",
        "    # Y = XW + B\n",
        "    # Y: output, X: input, W: weights, B: bias\n",
        "    self.input = input_data\n",
        "    self.output = np.dot(self.input, self.weights) + self.bias\n",
        "    return self.output\n",
        "\n",
        "  # Backward Propagation computes gradients and updates parameters\n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    # Compute gradient w.r.t input (dE/dX)\n",
        "    # input_error = dE/dY * W^T '''\n",
        "    input_error = np.dot(output_error, self.weights.T)\n",
        "    # Compute gradient w.r.t weights (dE/dW)\n",
        "    # weights_error = X^T * dE/dY\n",
        "    weights_error = np.dot(self.input.T, output_error)\n",
        "    # dBias = output_error\n",
        "\n",
        "    # Update parameters\n",
        "    # Adjust weights and bias by subtracting a fraction of the gradients\n",
        "    # Learning rate (lr) controls the size of the update step '''\n",
        "    self.weights -= learning_rate * weights_error\n",
        "    self.bias -= learning_rate * output_error\n",
        "    return input_error"
      ],
      "metadata": {
        "id": "RIshSnRfgT_-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding the Activation Layer ##"
      ],
      "metadata": {
        "id": "8wKxhSKYrUmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation class - inherits from base class Layer\n",
        "class ActivationLayer(Layer):\n",
        "  def __init__(self, activation, activation_prime):\n",
        "    self.activation = activation\n",
        "    self.activation_prime = activation_prime\n",
        "\n",
        "  # Forward propagation through the activation layer\n",
        "  # Uses the activation function on the input data and returns activated input\n",
        "  # Mathematically, this can be represented as: y = f(x)\n",
        "  def forward_propagation(self, input_data):\n",
        "    self.input = input_data\n",
        "    self.output = self.activation(self.input)\n",
        "    return self.output\n",
        "\n",
        "  # Backward propagation through the activation layer\n",
        "  # Calculates the gradient of the activation function with respect to the input\n",
        "  # Mathematically, it computes: dE/dX = f'(x) * dE/dY\n",
        "  # Here, f'(x) is the derivative of the activation function (activation_prime)\n",
        "  # and dE/dY is the output_error\n",
        "  # returns input_error for a fiven ouput_error\n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    return self.activation_prime(self.input) * output_error"
      ],
      "metadata": {
        "id": "OxEeV4nQrTHY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function ##"
      ],
      "metadata": {
        "id": "M-R9oRg5t4vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function and its deriv. -  put in sep. file\n",
        "\n",
        "# tanh activation function\n",
        "# Mathematically, this is: tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "# Derivative of the tanh function\n",
        "# Mathematically, this is: tanh'(x) = 1 - tanh(x)^2\n",
        "def tanh_prime(x):\n",
        "  return 1 - np.tanh(x)**2"
      ],
      "metadata": {
        "id": "qmEI6VSDttJ9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 python functions (loss and deriv. of loss) used when creating the network -  put in separate file\n",
        "\n",
        "# Mean Squared Error (MSE) function\n",
        "# Computes the average of the squares of the differences between actual (y_true) and predicted (y_pred) values.\n",
        "# Mathematically, MSE = 1/n * Σ(y_true - y_pred)^2, where n is the number of samplesdef mse(y_true, y_pred):\n",
        "def mse(y_true, y_pred):\n",
        "  return np.mean(np.power(y_true-y_pred, 2))\n",
        "\n",
        "# Derivative of the Mean Squared Error function\n",
        "# Used for calculating gradients during backpropagation\n",
        "# Mathematically, it is: d(MSE)/d(y_pred) = 2/n * Σ(y_pred - y_true)\n",
        "def mse_prime(y_true, y_pred):\n",
        "  return 2*(y_pred-y_true)/y_true.size"
      ],
      "metadata": {
        "id": "Y6STqgL1t4Cs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Class ##\n"
      ],
      "metadata": {
        "id": "PwPPvPuwvWtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.layers = [] # List of layers in the network\n",
        "    self.loss = None # Loss function\n",
        "    self.loss_prime = None # Derivative of loss function\n",
        "\n",
        "  # Method to add layer to network\n",
        "  # layer: An instance of a layer (e.g., ActivationLayer, DenseLayer)\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  # Method to set the loss function and its derivative\n",
        "  # loss: A function to calc. the loss\n",
        "  # loss_prime: A function to calc. the deriv. of the loss\n",
        "  def use(self, loss, loss_prime):\n",
        "    self.loss = loss\n",
        "    self.loss_prime = loss_prime\n",
        "\n",
        "  # Method to predict the output to given input data\n",
        "  # This performs the Forward Prop. through the network\n",
        "  def predit(self, input_data):\n",
        "    input_samples = len(input_data) # Number of input sample data\n",
        "    result = []\n",
        "\n",
        "    # Forward propagation through the network for each sample\n",
        "    for i in range(input_samples):\n",
        "      output = input_data[i]\n",
        "      for layer in self.layers:\n",
        "        output = layer.forward_propagation(output) # Getting output of curr. layer\n",
        "      result.append(output)\n",
        "\n",
        "    return result\n",
        "\n",
        "  # Method to train the network\n",
        "  # x_train: Training inputs\n",
        "  # y_train: Training outputs (labels)\n",
        "  # epochs: Number of time the entire training dataset is passed through the network\n",
        "  # learning_rate: Step size at each iteration of updating the weights\n",
        "  def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "    training_samples = len(x_train) # Number of training input sample data\n",
        "\n",
        "    # Training loop for the specified number of epochs\n",
        "    for i in range(epochs):\n",
        "      err = 0 # Error for the curr. epoch\n",
        "\n",
        "      # Forwardprop. and backprop. for each sample in the training set\n",
        "      for j in range(training_samples):\n",
        "        # Forward propagation\n",
        "        output = x_train[j]\n",
        "        for layer in self.layers:\n",
        "          output = layer.forward_propagation(output)\n",
        "\n",
        "        # Compute loss for the current sample (for display purposes)\n",
        "        err += self.loss(y_train[j], output)\n",
        "\n",
        "        # Backward propagation\n",
        "        error = self.loss_prime(y_train[j], output) # Compute the gradient of the loss\n",
        "        for layer in reversed(self.layers):\n",
        "          error = layer.backward_propagation(error, learning_rate) # Update the weights\n",
        "\n",
        "      # Calculate and print the average error over all samples for this epoch\n",
        "      err /= training_samples\n",
        "      print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
      ],
      "metadata": {
        "id": "nPRwDKtJvdHr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Neural Networks #\n"
      ],
      "metadata": {
        "id": "X-AVSc736AlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XOR Neural Network ##\n",
        "Simple way to tell if the NN is learning anything at all"
      ],
      "metadata": {
        "id": "CXTkpQiu6eqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data for XOR problem\n",
        "#\n",
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
        "\n",
        "# Creating the nn\n",
        "net = Network()\n",
        "\n",
        "# Adding a FC layer with 2 input neurons and 3 output neurons\n",
        "# This represents a weight matrix W of dimensions 3x2 and a bias vector b of length 3\n",
        "net.add(FCLayer(2, 3))\n",
        "# Adding a tanh activation layer\n",
        "# This applies the tanh activation function element-wise: y = tanh(x)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "# Adding another FC layer with 3 input neurons and 1 output neuron\n",
        "# Weight matrix W of dimensions 1x3 and bias vector b of length 1\n",
        "net.add(FCLayer(3, 1))\n",
        "# Adding another tanh activation layer\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "\n",
        "# Train the network\n",
        "# Setting the loss function to MSE and its derivative\n",
        "net.use(mse, mse_prime)\n",
        "# Training the network with the specified number of epochs and learning rate\n",
        "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
        "\n",
        "# Testing the network\n",
        "out = net.predit(x_train)\n",
        "print(out)\n",
        "\n",
        "# NOTE: I don’t think I need to emphasize many things. Just be careful\n",
        "# with the training data, you should always have the sample dimension first.\n",
        "# For example here, the input shape is (4,1,2)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVIrPmrH6AKx",
        "outputId": "60cdf627-4713-4a3b-dc20-e941075c7f9b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/1000   error=0.379323\n",
            "epoch 2/1000   error=0.310109\n",
            "epoch 3/1000   error=0.298394\n",
            "epoch 4/1000   error=0.294640\n",
            "epoch 5/1000   error=0.292911\n",
            "epoch 6/1000   error=0.291889\n",
            "epoch 7/1000   error=0.291176\n",
            "epoch 8/1000   error=0.290623\n",
            "epoch 9/1000   error=0.290164\n",
            "epoch 10/1000   error=0.289768\n",
            "epoch 11/1000   error=0.289417\n",
            "epoch 12/1000   error=0.289100\n",
            "epoch 13/1000   error=0.288812\n",
            "epoch 14/1000   error=0.288548\n",
            "epoch 15/1000   error=0.288305\n",
            "epoch 16/1000   error=0.288080\n",
            "epoch 17/1000   error=0.287872\n",
            "epoch 18/1000   error=0.287679\n",
            "epoch 19/1000   error=0.287500\n",
            "epoch 20/1000   error=0.287334\n",
            "epoch 21/1000   error=0.287179\n",
            "epoch 22/1000   error=0.287035\n",
            "epoch 23/1000   error=0.286901\n",
            "epoch 24/1000   error=0.286776\n",
            "epoch 25/1000   error=0.286660\n",
            "epoch 26/1000   error=0.286552\n",
            "epoch 27/1000   error=0.286451\n",
            "epoch 28/1000   error=0.286357\n",
            "epoch 29/1000   error=0.286270\n",
            "epoch 30/1000   error=0.286188\n",
            "epoch 31/1000   error=0.286113\n",
            "epoch 32/1000   error=0.286042\n",
            "epoch 33/1000   error=0.285977\n",
            "epoch 34/1000   error=0.285917\n",
            "epoch 35/1000   error=0.285860\n",
            "epoch 36/1000   error=0.285808\n",
            "epoch 37/1000   error=0.285760\n",
            "epoch 38/1000   error=0.285715\n",
            "epoch 39/1000   error=0.285674\n",
            "epoch 40/1000   error=0.285636\n",
            "epoch 41/1000   error=0.285600\n",
            "epoch 42/1000   error=0.285568\n",
            "epoch 43/1000   error=0.285538\n",
            "epoch 44/1000   error=0.285511\n",
            "epoch 45/1000   error=0.285486\n",
            "epoch 46/1000   error=0.285463\n",
            "epoch 47/1000   error=0.285443\n",
            "epoch 48/1000   error=0.285424\n",
            "epoch 49/1000   error=0.285407\n",
            "epoch 50/1000   error=0.285391\n",
            "epoch 51/1000   error=0.285378\n",
            "epoch 52/1000   error=0.285366\n",
            "epoch 53/1000   error=0.285355\n",
            "epoch 54/1000   error=0.285345\n",
            "epoch 55/1000   error=0.285337\n",
            "epoch 56/1000   error=0.285330\n",
            "epoch 57/1000   error=0.285324\n",
            "epoch 58/1000   error=0.285319\n",
            "epoch 59/1000   error=0.285316\n",
            "epoch 60/1000   error=0.285313\n",
            "epoch 61/1000   error=0.285311\n",
            "epoch 62/1000   error=0.285309\n",
            "epoch 63/1000   error=0.285309\n",
            "epoch 64/1000   error=0.285309\n",
            "epoch 65/1000   error=0.285310\n",
            "epoch 66/1000   error=0.285312\n",
            "epoch 67/1000   error=0.285314\n",
            "epoch 68/1000   error=0.285317\n",
            "epoch 69/1000   error=0.285320\n",
            "epoch 70/1000   error=0.285324\n",
            "epoch 71/1000   error=0.285328\n",
            "epoch 72/1000   error=0.285333\n",
            "epoch 73/1000   error=0.285338\n",
            "epoch 74/1000   error=0.285343\n",
            "epoch 75/1000   error=0.285349\n",
            "epoch 76/1000   error=0.285355\n",
            "epoch 77/1000   error=0.285362\n",
            "epoch 78/1000   error=0.285369\n",
            "epoch 79/1000   error=0.285376\n",
            "epoch 80/1000   error=0.285384\n",
            "epoch 81/1000   error=0.285391\n",
            "epoch 82/1000   error=0.285399\n",
            "epoch 83/1000   error=0.285408\n",
            "epoch 84/1000   error=0.285416\n",
            "epoch 85/1000   error=0.285425\n",
            "epoch 86/1000   error=0.285434\n",
            "epoch 87/1000   error=0.285443\n",
            "epoch 88/1000   error=0.285452\n",
            "epoch 89/1000   error=0.285461\n",
            "epoch 90/1000   error=0.285470\n",
            "epoch 91/1000   error=0.285480\n",
            "epoch 92/1000   error=0.285490\n",
            "epoch 93/1000   error=0.285499\n",
            "epoch 94/1000   error=0.285509\n",
            "epoch 95/1000   error=0.285519\n",
            "epoch 96/1000   error=0.285529\n",
            "epoch 97/1000   error=0.285539\n",
            "epoch 98/1000   error=0.285549\n",
            "epoch 99/1000   error=0.285559\n",
            "epoch 100/1000   error=0.285570\n",
            "epoch 101/1000   error=0.285580\n",
            "epoch 102/1000   error=0.285590\n",
            "epoch 103/1000   error=0.285600\n",
            "epoch 104/1000   error=0.285610\n",
            "epoch 105/1000   error=0.285620\n",
            "epoch 106/1000   error=0.285630\n",
            "epoch 107/1000   error=0.285640\n",
            "epoch 108/1000   error=0.285650\n",
            "epoch 109/1000   error=0.285659\n",
            "epoch 110/1000   error=0.285669\n",
            "epoch 111/1000   error=0.285678\n",
            "epoch 112/1000   error=0.285687\n",
            "epoch 113/1000   error=0.285696\n",
            "epoch 114/1000   error=0.285705\n",
            "epoch 115/1000   error=0.285713\n",
            "epoch 116/1000   error=0.285721\n",
            "epoch 117/1000   error=0.285729\n",
            "epoch 118/1000   error=0.285736\n",
            "epoch 119/1000   error=0.285743\n",
            "epoch 120/1000   error=0.285750\n",
            "epoch 121/1000   error=0.285756\n",
            "epoch 122/1000   error=0.285762\n",
            "epoch 123/1000   error=0.285767\n",
            "epoch 124/1000   error=0.285771\n",
            "epoch 125/1000   error=0.285775\n",
            "epoch 126/1000   error=0.285778\n",
            "epoch 127/1000   error=0.285780\n",
            "epoch 128/1000   error=0.285782\n",
            "epoch 129/1000   error=0.285782\n",
            "epoch 130/1000   error=0.285782\n",
            "epoch 131/1000   error=0.285780\n",
            "epoch 132/1000   error=0.285777\n",
            "epoch 133/1000   error=0.285773\n",
            "epoch 134/1000   error=0.285768\n",
            "epoch 135/1000   error=0.285761\n",
            "epoch 136/1000   error=0.285752\n",
            "epoch 137/1000   error=0.285742\n",
            "epoch 138/1000   error=0.285730\n",
            "epoch 139/1000   error=0.285716\n",
            "epoch 140/1000   error=0.285699\n",
            "epoch 141/1000   error=0.285681\n",
            "epoch 142/1000   error=0.285659\n",
            "epoch 143/1000   error=0.285635\n",
            "epoch 144/1000   error=0.285608\n",
            "epoch 145/1000   error=0.285578\n",
            "epoch 146/1000   error=0.285544\n",
            "epoch 147/1000   error=0.285507\n",
            "epoch 148/1000   error=0.285466\n",
            "epoch 149/1000   error=0.285420\n",
            "epoch 150/1000   error=0.285370\n",
            "epoch 151/1000   error=0.285315\n",
            "epoch 152/1000   error=0.285254\n",
            "epoch 153/1000   error=0.285188\n",
            "epoch 154/1000   error=0.285115\n",
            "epoch 155/1000   error=0.285035\n",
            "epoch 156/1000   error=0.284949\n",
            "epoch 157/1000   error=0.284854\n",
            "epoch 158/1000   error=0.284751\n",
            "epoch 159/1000   error=0.284639\n",
            "epoch 160/1000   error=0.284517\n",
            "epoch 161/1000   error=0.284385\n",
            "epoch 162/1000   error=0.284241\n",
            "epoch 163/1000   error=0.284085\n",
            "epoch 164/1000   error=0.283916\n",
            "epoch 165/1000   error=0.283732\n",
            "epoch 166/1000   error=0.283534\n",
            "epoch 167/1000   error=0.283318\n",
            "epoch 168/1000   error=0.283085\n",
            "epoch 169/1000   error=0.282833\n",
            "epoch 170/1000   error=0.282560\n",
            "epoch 171/1000   error=0.282264\n",
            "epoch 172/1000   error=0.281945\n",
            "epoch 173/1000   error=0.281600\n",
            "epoch 174/1000   error=0.281226\n",
            "epoch 175/1000   error=0.280822\n",
            "epoch 176/1000   error=0.280386\n",
            "epoch 177/1000   error=0.279915\n",
            "epoch 178/1000   error=0.279405\n",
            "epoch 179/1000   error=0.278854\n",
            "epoch 180/1000   error=0.278260\n",
            "epoch 181/1000   error=0.277617\n",
            "epoch 182/1000   error=0.276922\n",
            "epoch 183/1000   error=0.276171\n",
            "epoch 184/1000   error=0.275360\n",
            "epoch 185/1000   error=0.274483\n",
            "epoch 186/1000   error=0.273536\n",
            "epoch 187/1000   error=0.272511\n",
            "epoch 188/1000   error=0.271403\n",
            "epoch 189/1000   error=0.270205\n",
            "epoch 190/1000   error=0.268909\n",
            "epoch 191/1000   error=0.267508\n",
            "epoch 192/1000   error=0.265991\n",
            "epoch 193/1000   error=0.264349\n",
            "epoch 194/1000   error=0.262573\n",
            "epoch 195/1000   error=0.260649\n",
            "epoch 196/1000   error=0.258567\n",
            "epoch 197/1000   error=0.256312\n",
            "epoch 198/1000   error=0.253870\n",
            "epoch 199/1000   error=0.251226\n",
            "epoch 200/1000   error=0.248363\n",
            "epoch 201/1000   error=0.245265\n",
            "epoch 202/1000   error=0.241913\n",
            "epoch 203/1000   error=0.238289\n",
            "epoch 204/1000   error=0.234375\n",
            "epoch 205/1000   error=0.230153\n",
            "epoch 206/1000   error=0.225605\n",
            "epoch 207/1000   error=0.220716\n",
            "epoch 208/1000   error=0.215472\n",
            "epoch 209/1000   error=0.209864\n",
            "epoch 210/1000   error=0.203889\n",
            "epoch 211/1000   error=0.197547\n",
            "epoch 212/1000   error=0.190848\n",
            "epoch 213/1000   error=0.183811\n",
            "epoch 214/1000   error=0.176465\n",
            "epoch 215/1000   error=0.168847\n",
            "epoch 216/1000   error=0.161006\n",
            "epoch 217/1000   error=0.153002\n",
            "epoch 218/1000   error=0.144899\n",
            "epoch 219/1000   error=0.136767\n",
            "epoch 220/1000   error=0.128681\n",
            "epoch 221/1000   error=0.120712\n",
            "epoch 222/1000   error=0.112929\n",
            "epoch 223/1000   error=0.105393\n",
            "epoch 224/1000   error=0.098158\n",
            "epoch 225/1000   error=0.091268\n",
            "epoch 226/1000   error=0.084757\n",
            "epoch 227/1000   error=0.078649\n",
            "epoch 228/1000   error=0.072956\n",
            "epoch 229/1000   error=0.067683\n",
            "epoch 230/1000   error=0.062824\n",
            "epoch 231/1000   error=0.058367\n",
            "epoch 232/1000   error=0.054295\n",
            "epoch 233/1000   error=0.050585\n",
            "epoch 234/1000   error=0.047212\n",
            "epoch 235/1000   error=0.044149\n",
            "epoch 236/1000   error=0.041369\n",
            "epoch 237/1000   error=0.038845\n",
            "epoch 238/1000   error=0.036552\n",
            "epoch 239/1000   error=0.034467\n",
            "epoch 240/1000   error=0.032567\n",
            "epoch 241/1000   error=0.030833\n",
            "epoch 242/1000   error=0.029246\n",
            "epoch 243/1000   error=0.027792\n",
            "epoch 244/1000   error=0.026455\n",
            "epoch 245/1000   error=0.025225\n",
            "epoch 246/1000   error=0.024089\n",
            "epoch 247/1000   error=0.023038\n",
            "epoch 248/1000   error=0.022064\n",
            "epoch 249/1000   error=0.021159\n",
            "epoch 250/1000   error=0.020317\n",
            "epoch 251/1000   error=0.019531\n",
            "epoch 252/1000   error=0.018797\n",
            "epoch 253/1000   error=0.018110\n",
            "epoch 254/1000   error=0.017466\n",
            "epoch 255/1000   error=0.016861\n",
            "epoch 256/1000   error=0.016292\n",
            "epoch 257/1000   error=0.015756\n",
            "epoch 258/1000   error=0.015251\n",
            "epoch 259/1000   error=0.014773\n",
            "epoch 260/1000   error=0.014322\n",
            "epoch 261/1000   error=0.013894\n",
            "epoch 262/1000   error=0.013489\n",
            "epoch 263/1000   error=0.013104\n",
            "epoch 264/1000   error=0.012739\n",
            "epoch 265/1000   error=0.012391\n",
            "epoch 266/1000   error=0.012060\n",
            "epoch 267/1000   error=0.011744\n",
            "epoch 268/1000   error=0.011443\n",
            "epoch 269/1000   error=0.011156\n",
            "epoch 270/1000   error=0.010881\n",
            "epoch 271/1000   error=0.010618\n",
            "epoch 272/1000   error=0.010367\n",
            "epoch 273/1000   error=0.010126\n",
            "epoch 274/1000   error=0.009894\n",
            "epoch 275/1000   error=0.009673\n",
            "epoch 276/1000   error=0.009460\n",
            "epoch 277/1000   error=0.009255\n",
            "epoch 278/1000   error=0.009059\n",
            "epoch 279/1000   error=0.008869\n",
            "epoch 280/1000   error=0.008687\n",
            "epoch 281/1000   error=0.008512\n",
            "epoch 282/1000   error=0.008343\n",
            "epoch 283/1000   error=0.008180\n",
            "epoch 284/1000   error=0.008022\n",
            "epoch 285/1000   error=0.007870\n",
            "epoch 286/1000   error=0.007723\n",
            "epoch 287/1000   error=0.007581\n",
            "epoch 288/1000   error=0.007444\n",
            "epoch 289/1000   error=0.007312\n",
            "epoch 290/1000   error=0.007183\n",
            "epoch 291/1000   error=0.007059\n",
            "epoch 292/1000   error=0.006938\n",
            "epoch 293/1000   error=0.006821\n",
            "epoch 294/1000   error=0.006708\n",
            "epoch 295/1000   error=0.006598\n",
            "epoch 296/1000   error=0.006492\n",
            "epoch 297/1000   error=0.006388\n",
            "epoch 298/1000   error=0.006288\n",
            "epoch 299/1000   error=0.006190\n",
            "epoch 300/1000   error=0.006095\n",
            "epoch 301/1000   error=0.006003\n",
            "epoch 302/1000   error=0.005913\n",
            "epoch 303/1000   error=0.005826\n",
            "epoch 304/1000   error=0.005741\n",
            "epoch 305/1000   error=0.005658\n",
            "epoch 306/1000   error=0.005578\n",
            "epoch 307/1000   error=0.005499\n",
            "epoch 308/1000   error=0.005423\n",
            "epoch 309/1000   error=0.005348\n",
            "epoch 310/1000   error=0.005276\n",
            "epoch 311/1000   error=0.005205\n",
            "epoch 312/1000   error=0.005136\n",
            "epoch 313/1000   error=0.005068\n",
            "epoch 314/1000   error=0.005003\n",
            "epoch 315/1000   error=0.004938\n",
            "epoch 316/1000   error=0.004876\n",
            "epoch 317/1000   error=0.004814\n",
            "epoch 318/1000   error=0.004754\n",
            "epoch 319/1000   error=0.004696\n",
            "epoch 320/1000   error=0.004639\n",
            "epoch 321/1000   error=0.004583\n",
            "epoch 322/1000   error=0.004528\n",
            "epoch 323/1000   error=0.004475\n",
            "epoch 324/1000   error=0.004422\n",
            "epoch 325/1000   error=0.004371\n",
            "epoch 326/1000   error=0.004321\n",
            "epoch 327/1000   error=0.004272\n",
            "epoch 328/1000   error=0.004224\n",
            "epoch 329/1000   error=0.004177\n",
            "epoch 330/1000   error=0.004131\n",
            "epoch 331/1000   error=0.004086\n",
            "epoch 332/1000   error=0.004041\n",
            "epoch 333/1000   error=0.003998\n",
            "epoch 334/1000   error=0.003955\n",
            "epoch 335/1000   error=0.003914\n",
            "epoch 336/1000   error=0.003873\n",
            "epoch 337/1000   error=0.003833\n",
            "epoch 338/1000   error=0.003793\n",
            "epoch 339/1000   error=0.003755\n",
            "epoch 340/1000   error=0.003717\n",
            "epoch 341/1000   error=0.003680\n",
            "epoch 342/1000   error=0.003643\n",
            "epoch 343/1000   error=0.003608\n",
            "epoch 344/1000   error=0.003572\n",
            "epoch 345/1000   error=0.003538\n",
            "epoch 346/1000   error=0.003504\n",
            "epoch 347/1000   error=0.003471\n",
            "epoch 348/1000   error=0.003438\n",
            "epoch 349/1000   error=0.003406\n",
            "epoch 350/1000   error=0.003374\n",
            "epoch 351/1000   error=0.003343\n",
            "epoch 352/1000   error=0.003313\n",
            "epoch 353/1000   error=0.003283\n",
            "epoch 354/1000   error=0.003253\n",
            "epoch 355/1000   error=0.003224\n",
            "epoch 356/1000   error=0.003196\n",
            "epoch 357/1000   error=0.003168\n",
            "epoch 358/1000   error=0.003140\n",
            "epoch 359/1000   error=0.003113\n",
            "epoch 360/1000   error=0.003086\n",
            "epoch 361/1000   error=0.003060\n",
            "epoch 362/1000   error=0.003034\n",
            "epoch 363/1000   error=0.003008\n",
            "epoch 364/1000   error=0.002983\n",
            "epoch 365/1000   error=0.002958\n",
            "epoch 366/1000   error=0.002934\n",
            "epoch 367/1000   error=0.002910\n",
            "epoch 368/1000   error=0.002887\n",
            "epoch 369/1000   error=0.002863\n",
            "epoch 370/1000   error=0.002840\n",
            "epoch 371/1000   error=0.002818\n",
            "epoch 372/1000   error=0.002796\n",
            "epoch 373/1000   error=0.002774\n",
            "epoch 374/1000   error=0.002752\n",
            "epoch 375/1000   error=0.002731\n",
            "epoch 376/1000   error=0.002710\n",
            "epoch 377/1000   error=0.002689\n",
            "epoch 378/1000   error=0.002669\n",
            "epoch 379/1000   error=0.002649\n",
            "epoch 380/1000   error=0.002629\n",
            "epoch 381/1000   error=0.002610\n",
            "epoch 382/1000   error=0.002590\n",
            "epoch 383/1000   error=0.002571\n",
            "epoch 384/1000   error=0.002553\n",
            "epoch 385/1000   error=0.002534\n",
            "epoch 386/1000   error=0.002516\n",
            "epoch 387/1000   error=0.002498\n",
            "epoch 388/1000   error=0.002480\n",
            "epoch 389/1000   error=0.002463\n",
            "epoch 390/1000   error=0.002445\n",
            "epoch 391/1000   error=0.002428\n",
            "epoch 392/1000   error=0.002411\n",
            "epoch 393/1000   error=0.002395\n",
            "epoch 394/1000   error=0.002378\n",
            "epoch 395/1000   error=0.002362\n",
            "epoch 396/1000   error=0.002346\n",
            "epoch 397/1000   error=0.002330\n",
            "epoch 398/1000   error=0.002315\n",
            "epoch 399/1000   error=0.002299\n",
            "epoch 400/1000   error=0.002284\n",
            "epoch 401/1000   error=0.002269\n",
            "epoch 402/1000   error=0.002254\n",
            "epoch 403/1000   error=0.002240\n",
            "epoch 404/1000   error=0.002225\n",
            "epoch 405/1000   error=0.002211\n",
            "epoch 406/1000   error=0.002197\n",
            "epoch 407/1000   error=0.002183\n",
            "epoch 408/1000   error=0.002169\n",
            "epoch 409/1000   error=0.002155\n",
            "epoch 410/1000   error=0.002142\n",
            "epoch 411/1000   error=0.002129\n",
            "epoch 412/1000   error=0.002115\n",
            "epoch 413/1000   error=0.002102\n",
            "epoch 414/1000   error=0.002089\n",
            "epoch 415/1000   error=0.002077\n",
            "epoch 416/1000   error=0.002064\n",
            "epoch 417/1000   error=0.002052\n",
            "epoch 418/1000   error=0.002039\n",
            "epoch 419/1000   error=0.002027\n",
            "epoch 420/1000   error=0.002015\n",
            "epoch 421/1000   error=0.002003\n",
            "epoch 422/1000   error=0.001992\n",
            "epoch 423/1000   error=0.001980\n",
            "epoch 424/1000   error=0.001969\n",
            "epoch 425/1000   error=0.001957\n",
            "epoch 426/1000   error=0.001946\n",
            "epoch 427/1000   error=0.001935\n",
            "epoch 428/1000   error=0.001924\n",
            "epoch 429/1000   error=0.001913\n",
            "epoch 430/1000   error=0.001902\n",
            "epoch 431/1000   error=0.001891\n",
            "epoch 432/1000   error=0.001881\n",
            "epoch 433/1000   error=0.001870\n",
            "epoch 434/1000   error=0.001860\n",
            "epoch 435/1000   error=0.001850\n",
            "epoch 436/1000   error=0.001840\n",
            "epoch 437/1000   error=0.001830\n",
            "epoch 438/1000   error=0.001820\n",
            "epoch 439/1000   error=0.001810\n",
            "epoch 440/1000   error=0.001800\n",
            "epoch 441/1000   error=0.001791\n",
            "epoch 442/1000   error=0.001781\n",
            "epoch 443/1000   error=0.001772\n",
            "epoch 444/1000   error=0.001762\n",
            "epoch 445/1000   error=0.001753\n",
            "epoch 446/1000   error=0.001744\n",
            "epoch 447/1000   error=0.001735\n",
            "epoch 448/1000   error=0.001726\n",
            "epoch 449/1000   error=0.001717\n",
            "epoch 450/1000   error=0.001708\n",
            "epoch 451/1000   error=0.001699\n",
            "epoch 452/1000   error=0.001691\n",
            "epoch 453/1000   error=0.001682\n",
            "epoch 454/1000   error=0.001674\n",
            "epoch 455/1000   error=0.001665\n",
            "epoch 456/1000   error=0.001657\n",
            "epoch 457/1000   error=0.001649\n",
            "epoch 458/1000   error=0.001641\n",
            "epoch 459/1000   error=0.001633\n",
            "epoch 460/1000   error=0.001625\n",
            "epoch 461/1000   error=0.001617\n",
            "epoch 462/1000   error=0.001609\n",
            "epoch 463/1000   error=0.001601\n",
            "epoch 464/1000   error=0.001593\n",
            "epoch 465/1000   error=0.001586\n",
            "epoch 466/1000   error=0.001578\n",
            "epoch 467/1000   error=0.001571\n",
            "epoch 468/1000   error=0.001563\n",
            "epoch 469/1000   error=0.001556\n",
            "epoch 470/1000   error=0.001548\n",
            "epoch 471/1000   error=0.001541\n",
            "epoch 472/1000   error=0.001534\n",
            "epoch 473/1000   error=0.001527\n",
            "epoch 474/1000   error=0.001520\n",
            "epoch 475/1000   error=0.001513\n",
            "epoch 476/1000   error=0.001506\n",
            "epoch 477/1000   error=0.001499\n",
            "epoch 478/1000   error=0.001492\n",
            "epoch 479/1000   error=0.001485\n",
            "epoch 480/1000   error=0.001479\n",
            "epoch 481/1000   error=0.001472\n",
            "epoch 482/1000   error=0.001465\n",
            "epoch 483/1000   error=0.001459\n",
            "epoch 484/1000   error=0.001452\n",
            "epoch 485/1000   error=0.001446\n",
            "epoch 486/1000   error=0.001440\n",
            "epoch 487/1000   error=0.001433\n",
            "epoch 488/1000   error=0.001427\n",
            "epoch 489/1000   error=0.001421\n",
            "epoch 490/1000   error=0.001415\n",
            "epoch 491/1000   error=0.001408\n",
            "epoch 492/1000   error=0.001402\n",
            "epoch 493/1000   error=0.001396\n",
            "epoch 494/1000   error=0.001390\n",
            "epoch 495/1000   error=0.001384\n",
            "epoch 496/1000   error=0.001379\n",
            "epoch 497/1000   error=0.001373\n",
            "epoch 498/1000   error=0.001367\n",
            "epoch 499/1000   error=0.001361\n",
            "epoch 500/1000   error=0.001356\n",
            "epoch 501/1000   error=0.001350\n",
            "epoch 502/1000   error=0.001344\n",
            "epoch 503/1000   error=0.001339\n",
            "epoch 504/1000   error=0.001333\n",
            "epoch 505/1000   error=0.001328\n",
            "epoch 506/1000   error=0.001322\n",
            "epoch 507/1000   error=0.001317\n",
            "epoch 508/1000   error=0.001312\n",
            "epoch 509/1000   error=0.001306\n",
            "epoch 510/1000   error=0.001301\n",
            "epoch 511/1000   error=0.001296\n",
            "epoch 512/1000   error=0.001291\n",
            "epoch 513/1000   error=0.001285\n",
            "epoch 514/1000   error=0.001280\n",
            "epoch 515/1000   error=0.001275\n",
            "epoch 516/1000   error=0.001270\n",
            "epoch 517/1000   error=0.001265\n",
            "epoch 518/1000   error=0.001260\n",
            "epoch 519/1000   error=0.001255\n",
            "epoch 520/1000   error=0.001250\n",
            "epoch 521/1000   error=0.001246\n",
            "epoch 522/1000   error=0.001241\n",
            "epoch 523/1000   error=0.001236\n",
            "epoch 524/1000   error=0.001231\n",
            "epoch 525/1000   error=0.001227\n",
            "epoch 526/1000   error=0.001222\n",
            "epoch 527/1000   error=0.001217\n",
            "epoch 528/1000   error=0.001213\n",
            "epoch 529/1000   error=0.001208\n",
            "epoch 530/1000   error=0.001203\n",
            "epoch 531/1000   error=0.001199\n",
            "epoch 532/1000   error=0.001194\n",
            "epoch 533/1000   error=0.001190\n",
            "epoch 534/1000   error=0.001186\n",
            "epoch 535/1000   error=0.001181\n",
            "epoch 536/1000   error=0.001177\n",
            "epoch 537/1000   error=0.001173\n",
            "epoch 538/1000   error=0.001168\n",
            "epoch 539/1000   error=0.001164\n",
            "epoch 540/1000   error=0.001160\n",
            "epoch 541/1000   error=0.001156\n",
            "epoch 542/1000   error=0.001151\n",
            "epoch 543/1000   error=0.001147\n",
            "epoch 544/1000   error=0.001143\n",
            "epoch 545/1000   error=0.001139\n",
            "epoch 546/1000   error=0.001135\n",
            "epoch 547/1000   error=0.001131\n",
            "epoch 548/1000   error=0.001127\n",
            "epoch 549/1000   error=0.001123\n",
            "epoch 550/1000   error=0.001119\n",
            "epoch 551/1000   error=0.001115\n",
            "epoch 552/1000   error=0.001111\n",
            "epoch 553/1000   error=0.001107\n",
            "epoch 554/1000   error=0.001103\n",
            "epoch 555/1000   error=0.001099\n",
            "epoch 556/1000   error=0.001096\n",
            "epoch 557/1000   error=0.001092\n",
            "epoch 558/1000   error=0.001088\n",
            "epoch 559/1000   error=0.001084\n",
            "epoch 560/1000   error=0.001081\n",
            "epoch 561/1000   error=0.001077\n",
            "epoch 562/1000   error=0.001073\n",
            "epoch 563/1000   error=0.001070\n",
            "epoch 564/1000   error=0.001066\n",
            "epoch 565/1000   error=0.001062\n",
            "epoch 566/1000   error=0.001059\n",
            "epoch 567/1000   error=0.001055\n",
            "epoch 568/1000   error=0.001052\n",
            "epoch 569/1000   error=0.001048\n",
            "epoch 570/1000   error=0.001045\n",
            "epoch 571/1000   error=0.001041\n",
            "epoch 572/1000   error=0.001038\n",
            "epoch 573/1000   error=0.001035\n",
            "epoch 574/1000   error=0.001031\n",
            "epoch 575/1000   error=0.001028\n",
            "epoch 576/1000   error=0.001024\n",
            "epoch 577/1000   error=0.001021\n",
            "epoch 578/1000   error=0.001018\n",
            "epoch 579/1000   error=0.001014\n",
            "epoch 580/1000   error=0.001011\n",
            "epoch 581/1000   error=0.001008\n",
            "epoch 582/1000   error=0.001005\n",
            "epoch 583/1000   error=0.001002\n",
            "epoch 584/1000   error=0.000998\n",
            "epoch 585/1000   error=0.000995\n",
            "epoch 586/1000   error=0.000992\n",
            "epoch 587/1000   error=0.000989\n",
            "epoch 588/1000   error=0.000986\n",
            "epoch 589/1000   error=0.000983\n",
            "epoch 590/1000   error=0.000980\n",
            "epoch 591/1000   error=0.000977\n",
            "epoch 592/1000   error=0.000973\n",
            "epoch 593/1000   error=0.000970\n",
            "epoch 594/1000   error=0.000967\n",
            "epoch 595/1000   error=0.000964\n",
            "epoch 596/1000   error=0.000961\n",
            "epoch 597/1000   error=0.000959\n",
            "epoch 598/1000   error=0.000956\n",
            "epoch 599/1000   error=0.000953\n",
            "epoch 600/1000   error=0.000950\n",
            "epoch 601/1000   error=0.000947\n",
            "epoch 602/1000   error=0.000944\n",
            "epoch 603/1000   error=0.000941\n",
            "epoch 604/1000   error=0.000938\n",
            "epoch 605/1000   error=0.000935\n",
            "epoch 606/1000   error=0.000933\n",
            "epoch 607/1000   error=0.000930\n",
            "epoch 608/1000   error=0.000927\n",
            "epoch 609/1000   error=0.000924\n",
            "epoch 610/1000   error=0.000922\n",
            "epoch 611/1000   error=0.000919\n",
            "epoch 612/1000   error=0.000916\n",
            "epoch 613/1000   error=0.000913\n",
            "epoch 614/1000   error=0.000911\n",
            "epoch 615/1000   error=0.000908\n",
            "epoch 616/1000   error=0.000905\n",
            "epoch 617/1000   error=0.000903\n",
            "epoch 618/1000   error=0.000900\n",
            "epoch 619/1000   error=0.000898\n",
            "epoch 620/1000   error=0.000895\n",
            "epoch 621/1000   error=0.000892\n",
            "epoch 622/1000   error=0.000890\n",
            "epoch 623/1000   error=0.000887\n",
            "epoch 624/1000   error=0.000885\n",
            "epoch 625/1000   error=0.000882\n",
            "epoch 626/1000   error=0.000880\n",
            "epoch 627/1000   error=0.000877\n",
            "epoch 628/1000   error=0.000875\n",
            "epoch 629/1000   error=0.000872\n",
            "epoch 630/1000   error=0.000870\n",
            "epoch 631/1000   error=0.000867\n",
            "epoch 632/1000   error=0.000865\n",
            "epoch 633/1000   error=0.000862\n",
            "epoch 634/1000   error=0.000860\n",
            "epoch 635/1000   error=0.000858\n",
            "epoch 636/1000   error=0.000855\n",
            "epoch 637/1000   error=0.000853\n",
            "epoch 638/1000   error=0.000851\n",
            "epoch 639/1000   error=0.000848\n",
            "epoch 640/1000   error=0.000846\n",
            "epoch 641/1000   error=0.000844\n",
            "epoch 642/1000   error=0.000841\n",
            "epoch 643/1000   error=0.000839\n",
            "epoch 644/1000   error=0.000837\n",
            "epoch 645/1000   error=0.000834\n",
            "epoch 646/1000   error=0.000832\n",
            "epoch 647/1000   error=0.000830\n",
            "epoch 648/1000   error=0.000828\n",
            "epoch 649/1000   error=0.000825\n",
            "epoch 650/1000   error=0.000823\n",
            "epoch 651/1000   error=0.000821\n",
            "epoch 652/1000   error=0.000819\n",
            "epoch 653/1000   error=0.000817\n",
            "epoch 654/1000   error=0.000814\n",
            "epoch 655/1000   error=0.000812\n",
            "epoch 656/1000   error=0.000810\n",
            "epoch 657/1000   error=0.000808\n",
            "epoch 658/1000   error=0.000806\n",
            "epoch 659/1000   error=0.000804\n",
            "epoch 660/1000   error=0.000802\n",
            "epoch 661/1000   error=0.000800\n",
            "epoch 662/1000   error=0.000797\n",
            "epoch 663/1000   error=0.000795\n",
            "epoch 664/1000   error=0.000793\n",
            "epoch 665/1000   error=0.000791\n",
            "epoch 666/1000   error=0.000789\n",
            "epoch 667/1000   error=0.000787\n",
            "epoch 668/1000   error=0.000785\n",
            "epoch 669/1000   error=0.000783\n",
            "epoch 670/1000   error=0.000781\n",
            "epoch 671/1000   error=0.000779\n",
            "epoch 672/1000   error=0.000777\n",
            "epoch 673/1000   error=0.000775\n",
            "epoch 674/1000   error=0.000773\n",
            "epoch 675/1000   error=0.000771\n",
            "epoch 676/1000   error=0.000769\n",
            "epoch 677/1000   error=0.000767\n",
            "epoch 678/1000   error=0.000765\n",
            "epoch 679/1000   error=0.000763\n",
            "epoch 680/1000   error=0.000762\n",
            "epoch 681/1000   error=0.000760\n",
            "epoch 682/1000   error=0.000758\n",
            "epoch 683/1000   error=0.000756\n",
            "epoch 684/1000   error=0.000754\n",
            "epoch 685/1000   error=0.000752\n",
            "epoch 686/1000   error=0.000750\n",
            "epoch 687/1000   error=0.000748\n",
            "epoch 688/1000   error=0.000747\n",
            "epoch 689/1000   error=0.000745\n",
            "epoch 690/1000   error=0.000743\n",
            "epoch 691/1000   error=0.000741\n",
            "epoch 692/1000   error=0.000739\n",
            "epoch 693/1000   error=0.000738\n",
            "epoch 694/1000   error=0.000736\n",
            "epoch 695/1000   error=0.000734\n",
            "epoch 696/1000   error=0.000732\n",
            "epoch 697/1000   error=0.000730\n",
            "epoch 698/1000   error=0.000729\n",
            "epoch 699/1000   error=0.000727\n",
            "epoch 700/1000   error=0.000725\n",
            "epoch 701/1000   error=0.000723\n",
            "epoch 702/1000   error=0.000722\n",
            "epoch 703/1000   error=0.000720\n",
            "epoch 704/1000   error=0.000718\n",
            "epoch 705/1000   error=0.000717\n",
            "epoch 706/1000   error=0.000715\n",
            "epoch 707/1000   error=0.000713\n",
            "epoch 708/1000   error=0.000712\n",
            "epoch 709/1000   error=0.000710\n",
            "epoch 710/1000   error=0.000708\n",
            "epoch 711/1000   error=0.000707\n",
            "epoch 712/1000   error=0.000705\n",
            "epoch 713/1000   error=0.000703\n",
            "epoch 714/1000   error=0.000702\n",
            "epoch 715/1000   error=0.000700\n",
            "epoch 716/1000   error=0.000698\n",
            "epoch 717/1000   error=0.000697\n",
            "epoch 718/1000   error=0.000695\n",
            "epoch 719/1000   error=0.000693\n",
            "epoch 720/1000   error=0.000692\n",
            "epoch 721/1000   error=0.000690\n",
            "epoch 722/1000   error=0.000689\n",
            "epoch 723/1000   error=0.000687\n",
            "epoch 724/1000   error=0.000686\n",
            "epoch 725/1000   error=0.000684\n",
            "epoch 726/1000   error=0.000682\n",
            "epoch 727/1000   error=0.000681\n",
            "epoch 728/1000   error=0.000679\n",
            "epoch 729/1000   error=0.000678\n",
            "epoch 730/1000   error=0.000676\n",
            "epoch 731/1000   error=0.000675\n",
            "epoch 732/1000   error=0.000673\n",
            "epoch 733/1000   error=0.000672\n",
            "epoch 734/1000   error=0.000670\n",
            "epoch 735/1000   error=0.000669\n",
            "epoch 736/1000   error=0.000667\n",
            "epoch 737/1000   error=0.000666\n",
            "epoch 738/1000   error=0.000664\n",
            "epoch 739/1000   error=0.000663\n",
            "epoch 740/1000   error=0.000661\n",
            "epoch 741/1000   error=0.000660\n",
            "epoch 742/1000   error=0.000659\n",
            "epoch 743/1000   error=0.000657\n",
            "epoch 744/1000   error=0.000656\n",
            "epoch 745/1000   error=0.000654\n",
            "epoch 746/1000   error=0.000653\n",
            "epoch 747/1000   error=0.000651\n",
            "epoch 748/1000   error=0.000650\n",
            "epoch 749/1000   error=0.000649\n",
            "epoch 750/1000   error=0.000647\n",
            "epoch 751/1000   error=0.000646\n",
            "epoch 752/1000   error=0.000644\n",
            "epoch 753/1000   error=0.000643\n",
            "epoch 754/1000   error=0.000642\n",
            "epoch 755/1000   error=0.000640\n",
            "epoch 756/1000   error=0.000639\n",
            "epoch 757/1000   error=0.000637\n",
            "epoch 758/1000   error=0.000636\n",
            "epoch 759/1000   error=0.000635\n",
            "epoch 760/1000   error=0.000633\n",
            "epoch 761/1000   error=0.000632\n",
            "epoch 762/1000   error=0.000631\n",
            "epoch 763/1000   error=0.000629\n",
            "epoch 764/1000   error=0.000628\n",
            "epoch 765/1000   error=0.000627\n",
            "epoch 766/1000   error=0.000625\n",
            "epoch 767/1000   error=0.000624\n",
            "epoch 768/1000   error=0.000623\n",
            "epoch 769/1000   error=0.000621\n",
            "epoch 770/1000   error=0.000620\n",
            "epoch 771/1000   error=0.000619\n",
            "epoch 772/1000   error=0.000618\n",
            "epoch 773/1000   error=0.000616\n",
            "epoch 774/1000   error=0.000615\n",
            "epoch 775/1000   error=0.000614\n",
            "epoch 776/1000   error=0.000613\n",
            "epoch 777/1000   error=0.000611\n",
            "epoch 778/1000   error=0.000610\n",
            "epoch 779/1000   error=0.000609\n",
            "epoch 780/1000   error=0.000608\n",
            "epoch 781/1000   error=0.000606\n",
            "epoch 782/1000   error=0.000605\n",
            "epoch 783/1000   error=0.000604\n",
            "epoch 784/1000   error=0.000603\n",
            "epoch 785/1000   error=0.000601\n",
            "epoch 786/1000   error=0.000600\n",
            "epoch 787/1000   error=0.000599\n",
            "epoch 788/1000   error=0.000598\n",
            "epoch 789/1000   error=0.000597\n",
            "epoch 790/1000   error=0.000595\n",
            "epoch 791/1000   error=0.000594\n",
            "epoch 792/1000   error=0.000593\n",
            "epoch 793/1000   error=0.000592\n",
            "epoch 794/1000   error=0.000591\n",
            "epoch 795/1000   error=0.000589\n",
            "epoch 796/1000   error=0.000588\n",
            "epoch 797/1000   error=0.000587\n",
            "epoch 798/1000   error=0.000586\n",
            "epoch 799/1000   error=0.000585\n",
            "epoch 800/1000   error=0.000584\n",
            "epoch 801/1000   error=0.000582\n",
            "epoch 802/1000   error=0.000581\n",
            "epoch 803/1000   error=0.000580\n",
            "epoch 804/1000   error=0.000579\n",
            "epoch 805/1000   error=0.000578\n",
            "epoch 806/1000   error=0.000577\n",
            "epoch 807/1000   error=0.000576\n",
            "epoch 808/1000   error=0.000575\n",
            "epoch 809/1000   error=0.000573\n",
            "epoch 810/1000   error=0.000572\n",
            "epoch 811/1000   error=0.000571\n",
            "epoch 812/1000   error=0.000570\n",
            "epoch 813/1000   error=0.000569\n",
            "epoch 814/1000   error=0.000568\n",
            "epoch 815/1000   error=0.000567\n",
            "epoch 816/1000   error=0.000566\n",
            "epoch 817/1000   error=0.000565\n",
            "epoch 818/1000   error=0.000564\n",
            "epoch 819/1000   error=0.000562\n",
            "epoch 820/1000   error=0.000561\n",
            "epoch 821/1000   error=0.000560\n",
            "epoch 822/1000   error=0.000559\n",
            "epoch 823/1000   error=0.000558\n",
            "epoch 824/1000   error=0.000557\n",
            "epoch 825/1000   error=0.000556\n",
            "epoch 826/1000   error=0.000555\n",
            "epoch 827/1000   error=0.000554\n",
            "epoch 828/1000   error=0.000553\n",
            "epoch 829/1000   error=0.000552\n",
            "epoch 830/1000   error=0.000551\n",
            "epoch 831/1000   error=0.000550\n",
            "epoch 832/1000   error=0.000549\n",
            "epoch 833/1000   error=0.000548\n",
            "epoch 834/1000   error=0.000547\n",
            "epoch 835/1000   error=0.000546\n",
            "epoch 836/1000   error=0.000545\n",
            "epoch 837/1000   error=0.000544\n",
            "epoch 838/1000   error=0.000543\n",
            "epoch 839/1000   error=0.000542\n",
            "epoch 840/1000   error=0.000541\n",
            "epoch 841/1000   error=0.000540\n",
            "epoch 842/1000   error=0.000539\n",
            "epoch 843/1000   error=0.000538\n",
            "epoch 844/1000   error=0.000537\n",
            "epoch 845/1000   error=0.000536\n",
            "epoch 846/1000   error=0.000535\n",
            "epoch 847/1000   error=0.000534\n",
            "epoch 848/1000   error=0.000533\n",
            "epoch 849/1000   error=0.000532\n",
            "epoch 850/1000   error=0.000531\n",
            "epoch 851/1000   error=0.000530\n",
            "epoch 852/1000   error=0.000529\n",
            "epoch 853/1000   error=0.000528\n",
            "epoch 854/1000   error=0.000527\n",
            "epoch 855/1000   error=0.000526\n",
            "epoch 856/1000   error=0.000525\n",
            "epoch 857/1000   error=0.000524\n",
            "epoch 858/1000   error=0.000523\n",
            "epoch 859/1000   error=0.000522\n",
            "epoch 860/1000   error=0.000521\n",
            "epoch 861/1000   error=0.000521\n",
            "epoch 862/1000   error=0.000520\n",
            "epoch 863/1000   error=0.000519\n",
            "epoch 864/1000   error=0.000518\n",
            "epoch 865/1000   error=0.000517\n",
            "epoch 866/1000   error=0.000516\n",
            "epoch 867/1000   error=0.000515\n",
            "epoch 868/1000   error=0.000514\n",
            "epoch 869/1000   error=0.000513\n",
            "epoch 870/1000   error=0.000512\n",
            "epoch 871/1000   error=0.000511\n",
            "epoch 872/1000   error=0.000511\n",
            "epoch 873/1000   error=0.000510\n",
            "epoch 874/1000   error=0.000509\n",
            "epoch 875/1000   error=0.000508\n",
            "epoch 876/1000   error=0.000507\n",
            "epoch 877/1000   error=0.000506\n",
            "epoch 878/1000   error=0.000505\n",
            "epoch 879/1000   error=0.000504\n",
            "epoch 880/1000   error=0.000503\n",
            "epoch 881/1000   error=0.000503\n",
            "epoch 882/1000   error=0.000502\n",
            "epoch 883/1000   error=0.000501\n",
            "epoch 884/1000   error=0.000500\n",
            "epoch 885/1000   error=0.000499\n",
            "epoch 886/1000   error=0.000498\n",
            "epoch 887/1000   error=0.000497\n",
            "epoch 888/1000   error=0.000497\n",
            "epoch 889/1000   error=0.000496\n",
            "epoch 890/1000   error=0.000495\n",
            "epoch 891/1000   error=0.000494\n",
            "epoch 892/1000   error=0.000493\n",
            "epoch 893/1000   error=0.000492\n",
            "epoch 894/1000   error=0.000492\n",
            "epoch 895/1000   error=0.000491\n",
            "epoch 896/1000   error=0.000490\n",
            "epoch 897/1000   error=0.000489\n",
            "epoch 898/1000   error=0.000488\n",
            "epoch 899/1000   error=0.000487\n",
            "epoch 900/1000   error=0.000487\n",
            "epoch 901/1000   error=0.000486\n",
            "epoch 902/1000   error=0.000485\n",
            "epoch 903/1000   error=0.000484\n",
            "epoch 904/1000   error=0.000483\n",
            "epoch 905/1000   error=0.000483\n",
            "epoch 906/1000   error=0.000482\n",
            "epoch 907/1000   error=0.000481\n",
            "epoch 908/1000   error=0.000480\n",
            "epoch 909/1000   error=0.000479\n",
            "epoch 910/1000   error=0.000479\n",
            "epoch 911/1000   error=0.000478\n",
            "epoch 912/1000   error=0.000477\n",
            "epoch 913/1000   error=0.000476\n",
            "epoch 914/1000   error=0.000475\n",
            "epoch 915/1000   error=0.000475\n",
            "epoch 916/1000   error=0.000474\n",
            "epoch 917/1000   error=0.000473\n",
            "epoch 918/1000   error=0.000472\n",
            "epoch 919/1000   error=0.000472\n",
            "epoch 920/1000   error=0.000471\n",
            "epoch 921/1000   error=0.000470\n",
            "epoch 922/1000   error=0.000469\n",
            "epoch 923/1000   error=0.000469\n",
            "epoch 924/1000   error=0.000468\n",
            "epoch 925/1000   error=0.000467\n",
            "epoch 926/1000   error=0.000466\n",
            "epoch 927/1000   error=0.000466\n",
            "epoch 928/1000   error=0.000465\n",
            "epoch 929/1000   error=0.000464\n",
            "epoch 930/1000   error=0.000463\n",
            "epoch 931/1000   error=0.000463\n",
            "epoch 932/1000   error=0.000462\n",
            "epoch 933/1000   error=0.000461\n",
            "epoch 934/1000   error=0.000460\n",
            "epoch 935/1000   error=0.000460\n",
            "epoch 936/1000   error=0.000459\n",
            "epoch 937/1000   error=0.000458\n",
            "epoch 938/1000   error=0.000457\n",
            "epoch 939/1000   error=0.000457\n",
            "epoch 940/1000   error=0.000456\n",
            "epoch 941/1000   error=0.000455\n",
            "epoch 942/1000   error=0.000455\n",
            "epoch 943/1000   error=0.000454\n",
            "epoch 944/1000   error=0.000453\n",
            "epoch 945/1000   error=0.000452\n",
            "epoch 946/1000   error=0.000452\n",
            "epoch 947/1000   error=0.000451\n",
            "epoch 948/1000   error=0.000450\n",
            "epoch 949/1000   error=0.000450\n",
            "epoch 950/1000   error=0.000449\n",
            "epoch 951/1000   error=0.000448\n",
            "epoch 952/1000   error=0.000447\n",
            "epoch 953/1000   error=0.000447\n",
            "epoch 954/1000   error=0.000446\n",
            "epoch 955/1000   error=0.000445\n",
            "epoch 956/1000   error=0.000445\n",
            "epoch 957/1000   error=0.000444\n",
            "epoch 958/1000   error=0.000443\n",
            "epoch 959/1000   error=0.000443\n",
            "epoch 960/1000   error=0.000442\n",
            "epoch 961/1000   error=0.000441\n",
            "epoch 962/1000   error=0.000441\n",
            "epoch 963/1000   error=0.000440\n",
            "epoch 964/1000   error=0.000439\n",
            "epoch 965/1000   error=0.000439\n",
            "epoch 966/1000   error=0.000438\n",
            "epoch 967/1000   error=0.000437\n",
            "epoch 968/1000   error=0.000437\n",
            "epoch 969/1000   error=0.000436\n",
            "epoch 970/1000   error=0.000435\n",
            "epoch 971/1000   error=0.000435\n",
            "epoch 972/1000   error=0.000434\n",
            "epoch 973/1000   error=0.000433\n",
            "epoch 974/1000   error=0.000433\n",
            "epoch 975/1000   error=0.000432\n",
            "epoch 976/1000   error=0.000431\n",
            "epoch 977/1000   error=0.000431\n",
            "epoch 978/1000   error=0.000430\n",
            "epoch 979/1000   error=0.000429\n",
            "epoch 980/1000   error=0.000429\n",
            "epoch 981/1000   error=0.000428\n",
            "epoch 982/1000   error=0.000427\n",
            "epoch 983/1000   error=0.000427\n",
            "epoch 984/1000   error=0.000426\n",
            "epoch 985/1000   error=0.000426\n",
            "epoch 986/1000   error=0.000425\n",
            "epoch 987/1000   error=0.000424\n",
            "epoch 988/1000   error=0.000424\n",
            "epoch 989/1000   error=0.000423\n",
            "epoch 990/1000   error=0.000422\n",
            "epoch 991/1000   error=0.000422\n",
            "epoch 992/1000   error=0.000421\n",
            "epoch 993/1000   error=0.000421\n",
            "epoch 994/1000   error=0.000420\n",
            "epoch 995/1000   error=0.000419\n",
            "epoch 996/1000   error=0.000419\n",
            "epoch 997/1000   error=0.000418\n",
            "epoch 998/1000   error=0.000417\n",
            "epoch 999/1000   error=0.000417\n",
            "epoch 1000/1000   error=0.000416\n",
            "[array([[0.00096908]]), array([[0.97179532]]), array([[0.97063123]]), array([[-0.00034974]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST NN ##\n",
        "\n",
        "**Note:**\n",
        "We did not implement the Convolutional Layer but this is not a problem. All we need to do is to reshape our data so that it can fit into a Fully Connected Layer."
      ],
      "metadata": {
        "id": "C0QtrGNYAW42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDO-NnRODGPy",
        "outputId": "fd17a187-2297-498f-9d1a-4860b39af9b0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO implemented mini-batch GD\n"
      ],
      "metadata": {
        "id": "n0X3_eqJF2vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshape and normalize input data\n",
        "# Normalizing the data to the range of [0, 1] for faster convergence\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255  # Normalization: converting pixel values from [0, 255] to [0, 1]\n",
        "\n",
        "# One-hot encoding of the labels\n",
        "# Each digit label is converted into a binary vector of size 10\n",
        "# E.g., digit '3' becomes [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "\n",
        "# Repeat the process for test data\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Constructing the neural network for MNIST\n",
        "net = Network()\n",
        "# First fully connected layer: Input - 784 neurons (28*28 flattened image), Output - 100 neurons\n",
        "net.add(FCLayer(28*28, 100))\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "# Second fully connected layer: Input - 100 neurons, Output - 50 neurons\n",
        "net.add(FCLayer(100, 50))\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "# Third fully connected layer: Input - 50 neurons, Output - 10 neurons (corresponding to 10 digits)\n",
        "net.add(FCLayer(50, 10))\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "\n",
        "# Training the network with a smaller subset of the training data\n",
        "# Mini-batch gradient descent is not implemented, so we limit the number of samples\n",
        "net.use(mse, mse_prime)\n",
        "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
        "\n",
        "# Testing the network on a small subset of the test data\n",
        "out = net.predit(x_test[:3])\n",
        "print(f\"\\npredicted values:\\n{out}\\n\")\n",
        "print(f\"true values:\\n{y_test[:3]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPe4wbxNAWKp",
        "outputId": "c4be8775-2a8c-4e00-b642-fb6b60255bf8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/35   error=0.236787\n",
            "epoch 2/35   error=0.105192\n",
            "epoch 3/35   error=0.081818\n",
            "epoch 4/35   error=0.068472\n",
            "epoch 5/35   error=0.059351\n",
            "epoch 6/35   error=0.052295\n",
            "epoch 7/35   error=0.046089\n",
            "epoch 8/35   error=0.040928\n",
            "epoch 9/35   error=0.036414\n",
            "epoch 10/35   error=0.032838\n",
            "epoch 11/35   error=0.029981\n",
            "epoch 12/35   error=0.027767\n",
            "epoch 13/35   error=0.025839\n",
            "epoch 14/35   error=0.023879\n",
            "epoch 15/35   error=0.022149\n",
            "epoch 16/35   error=0.020705\n",
            "epoch 17/35   error=0.019360\n",
            "epoch 18/35   error=0.017992\n",
            "epoch 19/35   error=0.016651\n",
            "epoch 20/35   error=0.015557\n",
            "epoch 21/35   error=0.014572\n",
            "epoch 22/35   error=0.013660\n",
            "epoch 23/35   error=0.012933\n",
            "epoch 24/35   error=0.012216\n",
            "epoch 25/35   error=0.011505\n",
            "epoch 26/35   error=0.010914\n",
            "epoch 27/35   error=0.010346\n",
            "epoch 28/35   error=0.009857\n",
            "epoch 29/35   error=0.009385\n",
            "epoch 30/35   error=0.008924\n",
            "epoch 31/35   error=0.008539\n",
            "epoch 32/35   error=0.008212\n",
            "epoch 33/35   error=0.007874\n",
            "epoch 34/35   error=0.007584\n",
            "epoch 35/35   error=0.007326\n",
            "\n",
            "predicted values:\n",
            "[array([[-0.01310115,  0.02599995, -0.09657119, -0.09143739,  0.01139387,\n",
            "        -0.08703893, -0.0587256 ,  0.9795778 ,  0.20009264, -0.25958999]]), array([[ 0.31627753,  0.02889631, -0.01111478,  0.12020406, -0.01402726,\n",
            "         0.13820757,  0.2822629 ,  0.00856295,  0.17947933,  0.23445576]]), array([[-0.00375148,  0.97167048,  0.45069841, -0.07915071,  0.04478035,\n",
            "        -0.00521261, -0.25097105,  0.17810683, -0.05282072,  0.03202188]])]\n",
            "\n",
            "true values:\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
