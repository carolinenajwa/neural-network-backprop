# -*- coding: utf-8 -*-
"""custom_library_nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CCiIgeEgyYKtU1EiFI6sdS_QF6rXU7zp
"""

!pip install tensorflow

import unittest
import numpy as np
import tensorflow as tf

"""## Arithm. Functions ##"""

class ArithmeticFunctions:
  def __init__ (self):
    pass

  # dot function implemented
  def dot_product(self, a, b):
    """
    Compute the dot product of two 1-dimensional lists.

    Args:
    a (list): A 1-dimensional list of numbers.
    b (list): A 1-dimensional list of numbers.

    Returns:
    float: The dot product of the two lists.

    Raises:
    ValueError: If the input lists are not of equal length.
    """
    # Check if both lists are of the same length
    if len(a) != len(b):
      raise ValueError("Both lists must be of the same length")

    # Calculate the dot product
    dot_product = 0
    for i in range(len(a)):
      dot_product += a[i] * b[i]

    return dot_product

  def matrix_multiply(self, mat1, mat2):
    """ Helper function to perform matrix multiplication using dot_product """
    result = []
    for i in range(len(mat1)):
        result.append([])
        for j in range(len(mat2[0])):
            row = mat1[i]
            col = [mat2[k][j] for k in range(len(mat2))]
            result[-1].append(self.dot_product(row, col))
    return np.array(result)

  def compute_transpose(self, matrix):
    """
    Transpose a matrix.

    Args:
    matrix (list): A 2-dimensional list of numbers.

    Returns:
    list: The transposed matrix.
    """
    # Transpose the matrix
    if matrix.size == 0:
      return []
    transposed_matrix = []
    for i in range(len(matrix[0])):
      transposed_matrix.append([])
      for j in range(len(matrix)):
        transposed_matrix[i].append(matrix[j][i])

    return transposed_matrix

  def compute_factorial(self, n):
    """
    Compute the factorial of a number.

    Args:
    n (int): The number to compute the factorial for.

    Returns:
    int: The factorial of n.
    """
    if n == 0 or n == 1:
        return 1
    return n * self.compute_factorial(n - 1)

  # Source: https://ben.land/post/2021/02/24/power-without-math-lib/
  def compute_ipow(self, x,y):
    '''
    Args:
    x: int
    y: int

    Returns:
    int
    '''
    # Base case
    if y == 0:
        return 1
    # Positive case
    res = 1
    for i in range(y):
        # print(f"i: {i}") # Check
        # print(f"{res} * {x}") # Check
        res = res*x
        # print(f" = {res}") # Check
    return res

  def compute_exp(self, z, ftol=1e-15):
    """
    Compute the exponential of z (scalar or array) using a Taylor series expansion.

    Args:
    z: float or NumPy array
    ftol: float

    Returns:
    float or NumPy array
    """
    # Handling array input
    if isinstance(z, (list, np.ndarray)):
        return np.array([self.compute_exp(item, ftol) for item in z])

    # Handle scalar input
    # Base case
    if z == 0:
        return 1
    # Negative case
    if z < 0:
        return 1 / self.compute_exp(-z, ftol)

    # Positive case
    res = 1
    i = 1
    while True:
        term = self.compute_ipow(z, i) / self.compute_factorial(i)
        res += term
        if term < ftol:
            break
        i += 1

    return res

  # def test_compute_exp_scalar():
  #   arithmetic = ArithmeticFunctions()
  #   test_value = 2
  #   expected = np.exp(test_value)
  #   result = arithmetic.compute_exp(test_value)
  #   print(f"Result (Scalar): {result}, Expected: {expected}")
  #   if np.isclose(result, expected):
  #       print("Exp computation for scalar is correct")
  #   else:
  #       print("Exp computation for scalar is incorrect")


  # def test_compute_exp_array():
  #     arithmetic = ArithmeticFunctions()
  #     z_arr = np.array([-2, -1, 0, 1, 2])
  #     expected = np.exp(z_arr)
  #     result = arithmetic.compute_exp(z_arr)
  #     print(f"Result (Array): {result}, Expected: {expected}")
  #     if np.allclose(result, expected):
  #         print("Exp computation for array is correct")
  #     else:
  #         print("Exp computation for array is incorrect")

  # # Call the test functions
  # test_compute_exp_scalar()
  # test_compute_exp_array()

# @title class TestArithmeticFunctions

class TestArithmeticFunctions(unittest.TestCase):

  def setUp(self):
    self.arithmetic = ArithmeticFunctions()

  def test_dot_product(self):
    # Test for correct dot product
    self.assertEqual(self.arithmetic.dot_product([1, 2, 3], [4, 5, 6]), 32)
    # Test for ValueError when lists are of different lengths
    with self.assertRaises(ValueError):
        self.arithmetic.dot_product([1, 2], [1, 2, 3])

  # Corrected test_transpose (assuming it belongs to TestArithmeticFunctions)
  def test_transpose(self):
    matrix = [[1, 2, 3], [4, 5, 6]]
    expected = [[1, 4], [2, 5], [3, 6]]
    result = self.arithmetic.compute_transpose(matrix)  # Corrected method call
    self.assertEqual(result, expected)

  def test_transpose_regular_matrix(self):
    matrix = [[1, 2, 3], [4, 5, 6]]
    expected = [[1, 4], [2, 5], [3, 6]]
    result = self.arithmetic.compute_transpose(matrix)
    self.assertEqual(result, expected)

  def test_transpose_square_matrix(self):
    matrix = [[1, 2], [3, 4]]
    expected = [[1, 3], [2, 4]]
    result = self.arithmetic.compute_transpose(matrix)
    self.assertEqual(result, expected)

  def test_transpose_single_row(self):
    matrix = [[1, 2, 3]]
    expected = [[1], [2], [3]]
    result = self.arithmetic.compute_transpose(matrix)
    self.assertEqual(result, expected)

  def test_transpose_single_column(self):
    matrix = [[1], [2], [3]]
    expected = [[1, 2, 3]]
    result = self.arithmetic.compute_transpose(matrix)
    self.assertEqual(result, expected)

  # Corrected test_transpose_empty_matrix
  def test_transpose_empty_matrix(self):
    matrix = []
    expected = []
    result = self.arithmetic.compute_transpose(matrix)
    self.assertEqual(result, expected)

  def test_compute_factorial(self):
    self.assertEqual(self.arithmetic.compute_factorial(5), 120)
    self.assertEqual(self.arithmetic.compute_factorial(0), 1)

  def test_compute_ipow(self):
    self.assertEqual(self.arithmetic.compute_ipow(2, 3), 8)
    self.assertEqual(self.arithmetic.compute_ipow(5, 0), 1)

    # Corrected test_exp_array
  def test_exp_array(self):
    z_arr = np.array([-2, -1, 0, 1, 2])
    expected = np.exp(z_arr)
    result = self.arithmetic.compute_exp(z_arr)  # Fixed method call
    self.assertTrue(np.allclose(result, expected))

  # Corrected test_exp_scalar
  def test_exp_scalar(self):
    z = 2
    expected = np.exp(z)
    result = self.arithmetic.compute_exp(z)  # Fixed method call
    self.assertTrue(np.isclose(result, expected))


if __name__ == '__main__':
  unittest.main(argv=[''], verbosity=2, exit=False)

"""## Abstract Layer ##"""

# Abstract base class to handle input, output, forward prop., backward prop.
class Layer:
  """
    This class is an abstract base class for layers in the neural network.
  """
  def __init__(self):
    self.input = None
    self.output = None

  def forward_propagation(self, input):
    """
    Args:
      input: 2D array

    Returns:
      2D array
    """
    raise NotImplementedError

  def backward_propagation(self, output_error, learning_rate):
    """
    Args:
      output_error: 2D array
      learning_rate: float

    Returns:
      2D array
    """
    raise NotImplementedError

"""## Fully Connected Layer ##"""

arithmetic = ArithmeticFunctions()  # Object instantiation

class FCLayer(Layer):
  """
    This class implements a fully connected layer with an activation function
  """

  def __init__(self, input_size, output_size):
    """
    Args:
    input_size: The number of input neurons
    output_size: The number of output neurons
    """
    # Initialize weights and bias
    self.weights = np.random.rand(input_size, output_size) - 0.5
    self.bias = np.random.rand(1, output_size) - 0.5


  # def forward_propagation(self, input_data):
  #   """
  #   Args:
  #   input_data: 2D array

  #   Returns:
  #   2D array
  #   """
  #   # Y = XW + B
  #   self.input = input_data
  #   self.output = np.dot(self.input, self.weights) + self.bias
  #   return self.output

  def forward_propagation(self, input_data):
    # Y = XW + B
    self.input = input_data
    self.output = arithmetic.matrix_multiply(self.input, self.weights) + self.bias
    return self.output

  # def backward_propagation(self, output_error, learning_rate):
  #   """
  #   Args:
  #   output_error: 2D array
  #   learning_rate: float

  #   Returns:
  #   2D array
  #   """
  #   input_error = np.dot(output_error, self.weights.T) # Compute gradient w.r.t input (dE/dX)
  #   weights_error = np.dot(self.input.T, output_error) # Compute gradient w.r.t weights (dE/dW)
  #   # dBias = output_error

  #   # Update parameters
  #   self.weights -= learning_rate * weights_error
  #   self.bias -= learning_rate * output_error
  #   return input_error

  def backward_propagation(self, output_error, learning_rate):

    # Transpose weights and input using compute_transpose from ArithmeticFunctions
    transposed_weights = arithmetic.compute_transpose(self.weights)
    transposed_input = arithmetic.compute_transpose(self.input)

    # Use the transposed matrices in matrix multiplication
    input_error = arithmetic.matrix_multiply(output_error, transposed_weights)
    weights_error = arithmetic.matrix_multiply(transposed_input, output_error)

    # Update parameters
    self.weights -= learning_rate * weights_error
    self.bias -= learning_rate * output_error
    return input_error

"""## Activation Layer ##"""

class ActivationLayer(Layer):
  """
    This class implements an activation layer with the derivative of the activation function
  """

  def __init__(self, activation, activation_prime):
    '''
    Args:
    activation: The activation function to be used
    activation_prime: The derivative of the activation function
    '''
    self.activation = activation
    self.activation_prime = activation_prime

  def forward_propagation(self, input_data):
    """
    Args:
    input_data: 2D array

    Returns:
    2D array
    """
    # print("check1 : forward prop")
    self.input = input_data # Save the input for Back Prop.
    # print("check2 : forward prop")
    self.output = self.activation(self.input) # Compute the output of the activation function
    # print("check3 : forward prop")
    return self.output

  def backward_propagation(self, output_error, learning_rate):
    """
    Args:
    output_error: 2D array
    learning_rate: float

    Returns:
    2D array
    """
    return self.activation_prime(self.input) * output_error # dE/dX = f'(x) * dE/dY

"""## Loss Function ##"""

def compute_sin(x, n_terms=10):
  """
  Compute the sine of x using the Taylor series expansion.

  Args:
  x (float): The angle in radians.
  n_terms (int): The number of terms in the Taylor series to use.

  Returns:
  float: The approximate value of sin(x).
  """
  sine = 0
  for n in range(n_terms):
      sign = (-1) ** n
      sine += sign * (x ** (2 * n + 1)) / arithmetic.compute_factorial(2 * n + 1)
  return sine

def cos(x, n_terms=10):
  """
  Compute the cosine of x using the Taylor series expansion.

  Args:
  x (float): The angle in radians.
  n_terms (int): The number of terms in the Taylor series to use.

  Returns:
  float: The approximate value of cos(x).
  """
  cosine = 0
  for n in range(n_terms):
      sign = (-1) ** n
      cosine += sign * (x ** (2 * n)) / arithmetic.compute_factorial(2 * n)
  return cosine

def compute_tanh(x):
  """ Compute the hyperbolic tangent of x. """
  e_x = arithmetic.compute_exp(x)
  e_neg_x = arithmetic.compute_exp(-x)
  return (e_x - e_neg_x) / (e_x + e_neg_x)

def compute_tanh_prime(x):
  """ Compute the derivative of the hyperbolic tangent of x. """
  tanh_x = compute_tanh(x)
  return 1 - tanh_x**2

# def compute_sigmoid(x):
#   return 1/(1+compute_exp(-x))

# def compute_sigmoid_prime(x):
#   return compute_sigmoid(x)*(1-compute_sigmoid(x))

def mse(y_true, y_pred):
  return np.mean(np.power(y_true-y_pred, 2)) # MSE = 1/n * Σ(y_true - y_pred)^2

def mse_prime(y_true, y_pred):
  return 2*(y_pred-y_true)/y_true.size # d(MSE)/d(y_pred) = 2/n * Σ(y_pred - y_true)

"""## Network Class ##"""

class Network:
  def __init__(self):
    self.layers = [] # List of layers in the network
    self.loss = None # Loss function
    self.loss_prime = None # Derivative of loss function

  # Method to add layer to network
  # layer: An instance of a layer (e.g., ActivationLayer, DenseLayer)
  def add(self, layer):
    self.layers.append(layer)

      # Method to set the loss function and its derivative
  # loss: A function to calc. the loss
  # loss_prime: A function to calc. the deriv. of the loss
  def use(self, loss, loss_prime):
    self.loss = loss
    self.loss_prime = loss_prime

  # Method to predict the output to given input data
  # This performs the Forward Prop. through the network
  def predit(self, input_data):
    input_samples = len(input_data) # Number of input sample data
    result = []

    # Forward propagation through the network for each sample
    for i in range(input_samples):
      output = input_data[i]
      for layer in self.layers:
        output = layer.forward_propagation(output) # Getting output of curr. layer
      result.append(output)

    return result

  # Method to train the network
  # x_train: Training inputs
  # y_train: Training outputs (labels)
  # epochs: Number of time the entire training dataset is passed through the network
  # learning_rate: Step size at each iteration of updating the weights
  def fit(self, x_train, y_train, epochs, learning_rate):
    training_samples = len(x_train) # Number of training input sample data

    # Training loop for the specified number of epochs
    for i in range(epochs):
      err = 0 # Error for the curr. epoch

      # Forwardprop. and backprop. for each sample in the training set
      for j in range(training_samples):
        # Forward propagation
        output = x_train[j]
        for layer in self.layers:
          # print("check: fit")
          output = layer.forward_propagation(output)

        # Compute loss for the current sample (for display purposes)
        err += self.loss(y_train[j], output)

        # Backward propagation
        error = self.loss_prime(y_train[j], output) # Compute the gradient of the loss
        for layer in reversed(self.layers):
          error = layer.backward_propagation(error, learning_rate) # Update the weights

      # Calculate and print the average error over all samples for this epoch
      err /= training_samples
      print('epoch %d/%d   error=%f' % (i+1, epochs, err))

"""## Test Functions ##

# Building Neural Networks #
"""

x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])
y_train = np.array([[[0]], [[1]], [[1]], [[0]]])

net = Network()

# Adding a FC layer with 2 input neurons and 3 output neurons
# This represents a weight matrix W of dimensions 3x2 and a bias vector b of length 3
net.add(FCLayer(2, 3))
# Adding a tanh activation layer
# This applies the tanh activation function element-wise: y = tanh(x)
net.add(ActivationLayer(compute_tanh, compute_tanh_prime))
# Adding another FC layer with 3 input neurons and 1 output neuron
# Weight matrix W of dimensions 1x3 and bias vector b of length 1
net.add(FCLayer(3, 1))
# Adding another tanh activation layer
net.add(ActivationLayer(compute_tanh, compute_tanh_prime))

# Train the network
# Setting the loss function to MSE and its derivative
net.use(mse, mse_prime)
# Training the network with the specified number of epochs and learning rate
net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)

# Testing the network
out = net.predit(x_train)
print(out)